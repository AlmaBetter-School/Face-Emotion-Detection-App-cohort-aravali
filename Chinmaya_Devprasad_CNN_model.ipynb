{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chinmaya Devprasad CNN model",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinu1997/Face-Emotion-Detection-App/blob/master/Chinmaya_Devprasad_CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzX6ZsK1L_Vn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OwsHO8QtVyz",
        "outputId": "39b52e8e-8e4e-4bba-db79-d457e4cd2cfb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BJyp0MGkp3e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h81weyeHi9Y5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2\n",
        "from keras.preprocessing.image import load_img, img_to_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I46oqk2suvRl"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjKkxz5oYrGN",
        "outputId": "c1af8c79-77b2-4c6f-e0af-afde0f3d5738"
      },
      "source": [
        "pip install facial_emotion_recognition"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting facial_emotion_recognition\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/43/2359558c6ebdfa3a3625e72afb46ab17c06d473af0321f56fe81fe40fa23/facial_emotion_recognition-0.3.4.tar.gz (27.1MB)\n",
            "\u001b[K     |████████████████████████████████| 27.1MB 169kB/s \n",
            "\u001b[?25hCollecting facenet_pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e8/5ea742737665ba9396a8a2be3d2e2b49a13804b56a7e7bb101e8731ade8f/facenet_pytorch-2.5.2-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from facial_emotion_recognition) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from facial_emotion_recognition) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch->facial_emotion_recognition) (2.23.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch->facial_emotion_recognition) (7.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch->facial_emotion_recognition) (0.9.1+cu101)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch->facial_emotion_recognition) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch->facial_emotion_recognition) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch->facial_emotion_recognition) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch->facial_emotion_recognition) (2020.12.5)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet_pytorch->facial_emotion_recognition) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision->facenet_pytorch->facial_emotion_recognition) (3.7.4.3)\n",
            "Building wheels for collected packages: facial-emotion-recognition\n",
            "  Building wheel for facial-emotion-recognition (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for facial-emotion-recognition: filename=facial_emotion_recognition-0.3.4-cp37-none-any.whl size=27118755 sha256=678ee5fc606a54903a996d0fa5971d694201452f2b135d7b25c31071667dae74\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/25/9d/39471a4c71a562050c9d980a6724085dec9f0a92f1df50b89c\n",
            "Successfully built facial-emotion-recognition\n",
            "Installing collected packages: facenet-pytorch, facial-emotion-recognition\n",
            "Successfully installed facenet-pytorch-2.5.2 facial-emotion-recognition-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "EEWjefnFbOLe",
        "outputId": "c8c13962-45d8-48ab-ca34-7b81b2837f91"
      },
      "source": [
        "from facial_emotion_recognition import EmotionRecognition\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "\n",
        "from facial_emotion_recognition import EmotionRecognition\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torch\n",
        "er = EmotionRecognition(device='cpu', map_location=torch.device('cpu'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-af317f3838c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmotionRecognition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'map_location'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMP0VMZ8c4q1"
      },
      "source": [
        "vid=cv2.VideoCapture(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpLVWwqqc6EC"
      },
      "source": [
        "face_cascade=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8zu7Y6hxisi"
      },
      "source": [
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
        "from keras.models import Model,Sequential\n",
        "from keras.optimizers import Adam,SGD,RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SzIj4m0xnYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5920ee1a-27a3-42a7-e9e3-cde5903d9220"
      },
      "source": [
        "\n",
        "picture_size = 48\n",
        "folder_path =\"/content/drive/MyDrive/Face Emotion Detection/images/\"\n",
        "batch_size  = 128\n",
        "\n",
        "datagen_train  = ImageDataGenerator()\n",
        "datagen_val = ImageDataGenerator()\n",
        "\n",
        "train_set = datagen_train.flow_from_directory(\"/content/drive/MyDrive/Face Emotion Detection/images/train \",\n",
        "                                              target_size = (picture_size,picture_size),\n",
        "                                              color_mode = \"grayscale\",\n",
        "                                              batch_size=batch_size,\n",
        "                                              class_mode='categorical',\n",
        "                                              shuffle=True)\n",
        "\n",
        "\n",
        "test_set = datagen_val.flow_from_directory(\"/content/drive/MyDrive/Face Emotion Detection/images/validation\",\n",
        "                                              target_size = (picture_size,picture_size),\n",
        "                                              color_mode = \"grayscale\",\n",
        "                                              batch_size=batch_size,\n",
        "                                              class_mode='categorical',\n",
        "                                              shuffle=False)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7066 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zSsXN7I8kZC",
        "outputId": "8e46bb1a-5468-4a39-ce08-27a10dad9ea4"
      },
      "source": [
        "\n",
        "from keras.optimizers import Adam,SGD,RMSprop\n",
        "\n",
        "\n",
        "no_of_classes = 7\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#1st CNN layer\n",
        "model.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#2nd CNN layer\n",
        "model.add(Conv2D(128,(5,5),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout (0.25))\n",
        "\n",
        "#3rd CNN layer\n",
        "model.add(Conv2D(512,(3,3),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout (0.25))\n",
        "\n",
        "#4th CNN layer\n",
        "model.add(Conv2D(512,(3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully connected 1st layer\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# Fully connected layer 2nd layer\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(no_of_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "opt = Adam(lr = 0.0001)\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 24, 24, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 12, 12, 512)       590336    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 12, 12, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 6, 6, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 7)                 3591      \n",
            "=================================================================\n",
            "Total params: 4,478,727\n",
            "Trainable params: 4,474,759\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4BDTIok86wW"
      },
      "source": [
        "from keras.optimizers import RMSprop,SGD,Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/Face Emotion Detection/model.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                          min_delta=0,\n",
        "                          patience=3,\n",
        "                          verbose=1,\n",
        "                          restore_best_weights=True\n",
        "                          )\n",
        "\n",
        "reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.2,\n",
        "                              patience=3,\n",
        "                              verbose=1,\n",
        "                              min_delta=0.0001)\n",
        "\n",
        "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
        "\n",
        "epochs = 48\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = Adam(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQXKTbY78mQ9",
        "outputId": "521aa240-1f2d-454c-df3f-339735d6bee6"
      },
      "source": [
        "history = model.fit(train_set,steps_per_epoch=train_set.n//train_set.batch_size,epochs=epochs,\n",
        "                                validation_data = test_set,\n",
        "                                validation_steps = test_set.n//test_set.batch_size,\n",
        "                                callbacks=callbacks_list)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/48\n",
            "224/224 [==============================] - 60s 259ms/step - loss: 0.6971 - accuracy: 0.7396 - val_loss: 0.6975 - val_accuracy: 0.7479\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/48\n",
            "224/224 [==============================] - 57s 256ms/step - loss: 0.6339 - accuracy: 0.7658 - val_loss: 0.8136 - val_accuracy: 0.7036\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/48\n",
            "224/224 [==============================] - 57s 254ms/step - loss: 0.6115 - accuracy: 0.7704 - val_loss: 0.7660 - val_accuracy: 0.7209\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/48\n",
            "224/224 [==============================] - 57s 254ms/step - loss: 0.5556 - accuracy: 0.7952 - val_loss: 0.5924 - val_accuracy: 0.7970\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/48\n",
            "224/224 [==============================] - 58s 257ms/step - loss: 0.5335 - accuracy: 0.8028 - val_loss: 0.5712 - val_accuracy: 0.8168\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/48\n",
            "224/224 [==============================] - 57s 253ms/step - loss: 0.4656 - accuracy: 0.8311 - val_loss: 0.7416 - val_accuracy: 0.7428\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 7/48\n",
            "224/224 [==============================] - 57s 252ms/step - loss: 0.4611 - accuracy: 0.8325 - val_loss: 0.5844 - val_accuracy: 0.7925\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 8/48\n",
            "224/224 [==============================] - 56s 251ms/step - loss: 0.4135 - accuracy: 0.8508 - val_loss: 0.5407 - val_accuracy: 0.8270\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 9/48\n",
            "224/224 [==============================] - 56s 251ms/step - loss: 0.4046 - accuracy: 0.8515 - val_loss: 0.4229 - val_accuracy: 0.8756\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 10/48\n",
            "224/224 [==============================] - 57s 256ms/step - loss: 0.3655 - accuracy: 0.8663 - val_loss: 0.6751 - val_accuracy: 0.7940\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 11/48\n",
            "224/224 [==============================] - 57s 256ms/step - loss: 0.3454 - accuracy: 0.8743 - val_loss: 0.5653 - val_accuracy: 0.8261\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 12/48\n",
            "224/224 [==============================] - 57s 252ms/step - loss: 0.3328 - accuracy: 0.8781 - val_loss: 0.5244 - val_accuracy: 0.8415\n",
            "Restoring model weights from the end of the best epoch.\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMLH8FZQFHf"
      },
      "source": [
        "from google.colab import files\n",
        "import pickle as pi"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ45xYU7Q-BK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}